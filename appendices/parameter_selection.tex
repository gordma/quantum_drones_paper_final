\section{Parameter Selection and Ablation Studies}
\label{sec:appendix_parameters}

This appendix documents the comprehensive parameter sweeps and ablation studies that informed our final experimental design. These results justify the specific configurations used in the main comparison (Section~\ref{sec:results}) but are not used for headline performance claims.

\subsection{Genetic Algorithm Hyperparameter Optimization}
\label{sec:appendix_ga_tuning}

To ensure fair comparison, we conducted extensive mutation rate optimization for the GA baseline across $\mu \in \{0.15, 0.18, 0.20, 0.22, 0.25\}$ with 30 independent trials per configuration.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/01_ga_mutation_sweep.png}
  \caption{GA mutation rate sensitivity analysis. Each point represents mean cost $\pm 1\sigma$ over 30 trials. The optimal mutation rate $\mu^* = 0.22$ (highlighted region) was selected for all main comparisons. Performance degrades for $\mu > 0.25$ (excessive randomization) and $\mu < 0.15$ (premature convergence).}
  \label{fig:ga_mutation_sweep}
\end{figure}

Key findings:
\begin{itemize}
    \item \textbf{Optimal range}: $\mu \in [0.20, 0.25]$ provides consistent performance
    \item \textbf{Selected value}: $\mu^* = 0.22$ minimizes mean cost (8,415 $\pm$ 338)
    \item \textbf{Performance cliff}: Costs increase rapidly for $\mu > 0.25$ due to random walk behavior
    \item \textbf{Convergence issues}: $\mu < 0.18$ shows premature convergence and higher variance
\end{itemize}

This analysis ensures our GA baseline represents genuinely competitive classical performance rather than an easily-beaten strawman.

\subsection{VQE Parameter Space Exploration}
\label{sec:appendix_vqe_grid}

We performed a systematic grid search across CVaR risk parameters $\alpha \in \{0.03, 0.05, 0.07, 0.10, 0.50, 1.0\}$ and circuit depths $L \in \{1, 2, 5\}$ to identify the most promising quantum configurations.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/02_vqe_alpha_layer_heatmap.png}
  \caption{VQE parameter space heatmap showing cost advantage versus GA baseline. Darker green indicates better performance. The optimal region (α ≈ 0.05, L ≤ 2) is highlighted with thick borders. Hatched cells indicate configurations with fewer than 30 trials.}
  \label{fig:vqe_parameter_grid}
\end{figure}

Parameter selection insights:
\begin{itemize}
    \item \textbf{Risk parameter}: $\alpha = 0.05$ consistently optimal across circuit depths
    \item \textbf{Shallow circuits}: $L \leq 2$ dominate the performance landscape
    \item \textbf{Diminishing returns}: $L = 5$ shows no improvement despite 4× parameter increase
    \item \textbf{CVaR effectiveness}: $\alpha = 1.0$ (expectation) underperforms risk-aware settings
    \item \textbf{Robustness}: Best configurations show 600+ point advantage over GA baseline
\end{itemize}

These results motivated our selection of $\alpha = 0.05$ and $L \in \{1, 2\}$ for the final comparison study.

\subsection{Bayesian Optimizer Ablation}
\label{sec:appendix_optimizer_ablation}

We compared three Bayesian optimization strategies to validate that our quantum advantage is not merely due to superior hyperparameter optimization:

% This table would be generated from experiments/vqe_final/ data
\input{tables/latex_fragments/04_optimizer_ablation.tex}

Optimizer comparison findings:
\begin{itemize}
    \item \textbf{Consistent advantage}: All quantum optimizers (Aggressive-TPE, Default-TPE, Random) outperform GA by 5-7\%
    \item \textbf{Robustness to hyperparameters}: Default-TPE nearly matches Aggressive-TPE performance
    \item \textbf{CVaR effectiveness}: Even random search performs competitively due to CVaR filtering
    \item \textbf{Exploitation benefit}: Aggressive-TPE provides marginal improvement for tail-focused CVaR
\end{itemize}

This analysis confirms that quantum advantage persists across optimization strategies and is not an artifact of hyperparameter tuning.

\subsection{Extended Budget Analysis}
\label{sec:appendix_extended_budget}

To validate scaling behavior, we conducted extended runs with 2M evaluation budgets:

\input{tables/latex_fragments/02_budget_scaling.tex}

Budget scaling observations:
\begin{itemize}
    \item \textbf{Persistent advantage}: Quantum superiority maintained across budget scales
    \item \textbf{Diminishing gaps}: Advantage narrows slightly with increased evaluations
    \item \textbf{Efficiency implications}: VQE extracts more value per evaluation
    \item \textbf{Practical relevance}: 200k budget represents realistic deployment constraints
\end{itemize}

\subsection{Statistical Validation}
\label{sec:appendix_statistics}

All comparisons employ rigorous statistical testing:
\begin{itemize}
    \item \textbf{Sample sizes}: 30 independent trials minimum for all configurations
    \item \textbf{Significance testing}: One-sided Welch t-tests for unequal variances
    \item \textbf{Effect sizes}: Cohen's d reported for practical significance
    \item \textbf{Multiple comparisons}: Bonferroni correction applied where appropriate
\end{itemize}

Power analysis confirms our sample sizes provide >80\% power to detect 5\% performance differences at $\alpha = 0.05$.

\subsection{Implementation Details}
\label{sec:appendix_implementation}

\textbf{Computational resources}: All experiments executed on a Macbook Pro M4 Pro with 48GB RAM.

\textbf{Random seed management}: Seeds 0-29 used consistently across methods for reproducibility

\textbf{Convergence criteria}: Median pruning with 90\% retention prevents premature termination

\textbf{Route generation}: Deterministic seed ensures identical problem instances across all comparisons 